realminusworld observations such as the measurements of yesterday's rain throughout the day typically cannot be complete sets of all possible observations that could be made. as such, the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations. this means that one estimates the mean and variance that would have been calculated from an omniscient set of observations by using an estimator equation. the estimator is a function of the sample of n observations drawn without observational bias from the whole population of potential observations. in this example that sample would be the set of actual measurements of yesterday's rainfall from available rain gauges within the geography of interest.

the simplest estimators for population mean and population variance are simply the mean and variance of the sample, the sample mean and (uncorrected) sample variance, these are consistent estimators (they converge to the correct value as the number of samples increases), but can be improved. estimating the population variance by taking the sample's variance is close to optimal in general, but can be improved in two ways. most simply, the sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by n. however, using values other than n improves the estimator in various ways. four common values for the denominator are n, n minus 1, n plus 1, and n minus 1.5: n is the simplest (population variance of the sample), n minus 1 eliminates bias, n plus 1 minimizes mean squared error for the normal distribution, and n minus 1.5 mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.

firstly, if the omniscient mean is unknown (and is computed as the sample mean), then the sample variance is a biased estimator: it underestimates the variance by a factor of (n minus 1) divde by n; correcting by this factor (dividing by n minus 1 instead of n) is called bessel's correction. the resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. for example, when n equal 1 the variance of a single observation about the sample mean (itself) is obviously zero regardless of the population variance. if the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.

secondly, the sample variance does not generally minimize mean squared error between sample variance and population variance. correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see mean squared error: variance), and introduces bias. this always consists of scaling down the unbiased estimator (dividing by a number larger than n minus 1), and is a simple example of a shrinkage estimator: one "shrinks" the unbiased estimator towards zero. for the normal distribution, dividing by n plus 1 (instead of n minus 1 or n) minimizes mean squared error. the resulting estimator is biased, however, and is known as the biased sample variation.