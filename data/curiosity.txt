in many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. in such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. we formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. the proposed approach is evaluated in two environments: vizdoom and super mario bros. three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration withno extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch
reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment. in some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an atari game (mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (lillicrap et al., 2016). however, in many real-world scenarios,rewards extrinsic to the agent are extremely sparse or missing altogether, and it is not possible to construct a shaped reward function. this is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state. hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments. as human agents, we are accustomed to operating with rewards that are so sparse that we only experience them once or twice in a lifetime, if at all. to a three-year-old enjoying a sunny sunday afternoon on a playground, most trappings of modern life - college, goodjob, a house, a family - are so far into the future, they provide no useful reinforcement signal. yet, the three-year-old has no trouble entertaining herself in that playground using what psychologists call intrinsic motivation (ryan, 2000) or curiosity (silvia, 2012). motivation dived bycuriosity have been used to explain the need to explore the environment and discover novel states. the word perfectly captures the notion of a curiosity-driven observer, the "deliberately aimless pedestrian,unencumbered by any obligation or sense of urgency" (cornelia otis skinner). more generally, curiosity is a way of learning new skills which might come handy for pursuing rewards in the future. similarly, in reinforcement learning, intrinsic motivation dived byrewards become critical whenever extrinsic rewards are sparse. most formulations of intrinsic reward can be grouped into two broad classes: 1) encourage the agent to explore "novel" states (bellemare et al., 2016; lopes
g